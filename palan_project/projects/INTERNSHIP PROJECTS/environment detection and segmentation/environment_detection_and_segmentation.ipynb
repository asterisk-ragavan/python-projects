{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFPcBuVFw61h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Driving Scene Segmentation\n",
    "\n",
    "This project demostrates the steps to run DeepLab semantic scene segmentation model on a sample image from MIT Driving Scene Segmentation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kAbdmRmvq0Je",
    "outputId": "f82ca270-f776-41b7-c542-dbf02116015c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import warnings\n",
    "import dlm\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQ4WfdFGKLSc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build the model\n",
    "\n",
    "**[DeepLab](https://github.com/tensorflow/models/tree/master/research/deeplab)** is a state-of-art deep learning model for semantic image segmentation, where the goal is to assign semantic labels (e.g., person, dog, cat and so on) to every pixel in the input image. Some segmentation results on Flickr images:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/img/vis1.png?raw=true\" width=600></br>\n",
    "    <img src=\"https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/img/vis2.png?raw=true\" width=600></br>\n",
    "</p>\n",
    "\n",
    "In the driving context, we aim to obtain a semantic understanding of the front driving scene throught the camera input. This is important for driving safety and an essential requirement for all levels of autonomous driving. The first step is to build the model and load the pre-trained weights. In this demo, we use the model checkpoint trained on [Cityscapes](https://www.cityscapes-dataset.com/) dataset.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/muenster00.png\" width=600></br>\n",
    "    <img src=\"https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/zuerich00.png\" width=600></br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixa_Cty2KLSc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
    "        target_size = (2049,1025)  # size of Cityscapes images\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
    "        if len(seg_map.shape) == 2:\n",
    "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
    "        seg_map = cv.resize(seg_map, (image.size), interpolation=cv.INTER_NEAREST)\n",
    "        return seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzxbIveNKLSi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the model from a frozen graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "c4oXKmnjw6i_",
    "outputId": "19f93d51-1e69-4b7d-b9b5-16affad308b2",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "download_path = r'C:\\Users\\sakth\\PycharmProjects\\python-projects\\palan_project\\projects\\INTERNSHIP PROJECTS\\environment detection and segmentation\\deeplab_model.tar.gz'\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZst78N-4OKO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run on the sample image\n",
    "The sample image is frame #0 in the MIT Driving Scene Segmentation (DriveSeg) Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "edGukUHXyymr",
    "outputId": "68858de4-7d6f-464c-bfed-b576a2843ea3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running deeplab on the sample image...\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_IMAGE = r'C:\\Users\\sakth\\PycharmProjects\\python-projects\\palan_project\\projects\\INTERNSHIP PROJECTS\\environment detection and segmentation\\environment_detection_and_segmentation.png'\n",
    "print('running deeplab on the sample image...')\n",
    "#dlm.run_visualization(SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running deeplab on the sample image...\n"
     ]
    }
   ],
   "source": [
    "TEST_IMAGE = r'C:\\Users\\sakth\\PycharmProjects\\python-projects\\palan_project\\projects\\INTERNSHIP PROJECTS\\environment detection and segmentation\\test_local.jpg'\n",
    "print('running deeplab on the sample image...')\n",
    "#dlm.run_visualization(TEST_IMAGE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_driving_scene_segmentation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}